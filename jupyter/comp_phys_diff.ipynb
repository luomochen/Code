{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二章 计算机上的微分\n",
    "\n",
    "可分为三类，分别是：\n",
    "- 解析微分 如Sympy和Mathematica\n",
    "- 数值微分：有限差分近似 （Finite Difference）\n",
    "- 自动微分：科学计算与人工智能的交叉 (Auto Differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析微分\n",
    "\n",
    "- Step1: 定义自变量符号\n",
    "- Step2: 定义解析函数的形式\n",
    "- Step3: 使用 sympy.diff 函数返回函数的微分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x e^{- x^{2}}$"
      ],
      "text/plain": [
       "x*exp(-x**2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy \n",
    "\n",
    "# Step1: define the variable\n",
    "x = sympy.symbols(\"x\")\n",
    "# Step2: define the function \n",
    "def f(x):\n",
    "    return x * sympy.exp(- x**2)\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 x^{2} e^{- x^{2}} + e^{- x^{2}}$"
      ],
      "text/plain": [
       "-2*x**2*exp(-x**2) + exp(-x**2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step3: calc the differentiation\n",
    "sympy.diff(f(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 2 x^{2} e^{- x^{2}} + e^{- x^{2}}\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1$"
      ],
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx = sympy.diff(f(x), x)\n",
    "# Set the output type to LaTeX.\n",
    "sympy.print_latex(dfdx)\n",
    "# The subs can Temporarily replace the x to a number.\n",
    "dfdx.subs(x, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数值微分\n",
    "有限差分近似：${df \\over dx} = {\\rm lim}_{\\Delta x\\rightarrow 0} {f(x+ \\Delta x) - f(x) \\over \\Delta x }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.999900004999833$"
      ],
      "text/plain": [
       "0.999900004999833"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finite_difference(func, x, dx=0.01):\n",
    "    assert(dx != 0)\n",
    "    return (func(x+dx) - func(x)) / dx\n",
    "\n",
    "finite_difference(f, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有限差分法的原理（泰勒展开）\n",
    "\n",
    "$f(x + \\Delta x) = f(x) + {df \\over dx} \\Delta x + {d^2 f \\over dx^2} {\\Delta x^2 \\over 2} + ...$\n",
    "\n",
    "移项得到一阶精度的前向有限差分近似，\n",
    "\n",
    "${df \\over dx}  = {f(x + \\Delta x) - f(x) \\over \\Delta x} + O( \\Delta x^2)$\n",
    "\n",
    "截断误正比于 $\\Delta x^2$, 所以 $\\Delta x$ 越小结果越精确。\n",
    "\n",
    "同理得到后向有限差分近似，\n",
    "\n",
    "$f(x - \\Delta x) = f(x) - {df \\over dx} \\Delta x + {d^2 f \\over dx^2} {\\Delta x^2 \\over 2} + ...$\n",
    "\n",
    "移项得到一阶精度的后向有限差分近似，\n",
    "\n",
    "${df \\over dx}  = {f(x) - f(x - \\Delta x) \\over \\Delta x} + O( \\Delta x^2)$\n",
    "\n",
    "前向与后向有限差分，有区别吗？\n",
    "\n",
    "答案是： Yes\n",
    "\n",
    "在偏微分方程的数值解中，有时候要构造“迎风法”，使得数值解稳定。举例：\n",
    "\n",
    "${\\partial f \\over \\partial t} + u {\\partial f \\over \\partial x} = 0$\n",
    "\n",
    "迎风法考虑信号不能超光速传播，根据对流速度 u 选择前向还是后向差分。\n",
    "\n",
    "- u>0, 选择后向差分\n",
    "- u<0, 选择前向差分\n",
    "\n",
    "中心差分格式（二阶精度）\n",
    "\n",
    "$f(x + \\Delta x) = f(x) + {df \\over dx} \\Delta x + {d^2 f \\over dx^2} {\\Delta x^2 \\over 2} + ...$\n",
    "\n",
    "$f(x - \\Delta x) = f(x) - {df \\over dx} \\Delta x + {d^2 f \\over dx^2} {\\Delta x^2 \\over 2} + ...$\n",
    "\n",
    "将上面两个泰勒展开公式相减，可以得到二阶精度的中心差分格式：\n",
    "\n",
    "${df \\over dx}  = {f(x + \\Delta x) - f(x - \\Delta x) \\over 2 \\Delta x} + O( \\Delta x^3)$\n",
    "\n",
    "二阶精度如果在边界处不可用，还是得考虑一阶精度。\n",
    "\n",
    "如果考虑四个泰勒展开公式，(其中 $h=\\Delta x$)：\n",
    "\n",
    "\n",
    "$f(x - 2 h) =  f(x) - 2 {df \\over dx} h + {d^2 f \\over dx^2} {4 h^2 \\over 2} - {d^3 f \\over dx^3} {8 h^3 \\over 6} + ... (1)$\n",
    "\n",
    "$f(x - h) = f(x) - {df \\over dx} h + {d^2 f \\over dx^2} {h^2 \\over 2} - {d^3 f \\over dx^3} { h^3 \\over 6} +... (2)$\n",
    "\n",
    "$f(x + h) = f(x) + {df \\over dx} h + {d^2 f \\over dx^2} {h^2 \\over 2} + {d^3 f \\over dx^3} { h^3 \\over 6} +... (3)$\n",
    "\n",
    "$f(x + 2 h) = f(x) + 2 {df \\over dx} h + {d^2 f \\over dx^2} {4 h \\over 2} + {d^3 f \\over dx^3} {8 h^3 \\over 6} + ... (4)$\n",
    "\n",
    "利用$8\\times((3)-(2))-((4)-(1))$，可以得到三阶精度的差分公式：\n",
    "\n",
    "${df \\over dx}  = {8(f(x + \\Delta x) - f(x - \\Delta x)) - f(x + 2 \\Delta x) + f(x - 2 \\Delta x) \\over 12 \\Delta x} + O( \\Delta x^4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df382b6c76e4d5b80b777f7d8cd4c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5005, description='dx', max=1.0, min=0.001), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.comparison(dx)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 中心差分法\n",
    "def central_difference(func, x, dx=0.01):\n",
    "    \"\"\"\n",
    "    o(x^3)\n",
    "    \"\"\"\n",
    "    assert(dx != 0)\n",
    "    return (func(x+dx) - func(x-dx)) / 2 / dx\n",
    "\n",
    "def three_order_difference(func, x, dx=0.01):\n",
    "    \"\"\"\n",
    "    o(x^4)\n",
    "    \"\"\"\n",
    "    assert(dx !=0)\n",
    "    return (8*(func(x+dx) - func(x-dx))-(func(x+2*dx)-func(x-2*dx))) / 12 / dx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "# 这里作图看一下有限差分方法的误差\n",
    "def comparison(dx):\n",
    "    xcoord = np.linspace(0, np.pi, 20)\n",
    "    # 解析微分的结果\n",
    "    dfdx_ana = [dfdx.subs(x, xi) for xi in xcoord]\n",
    "    # 有限差分的结果\n",
    "    dfdx_num = [finite_difference(f, xi, dx) for xi in xcoord]\n",
    "    # 中心差分的结果\n",
    "    dfdx_cen_num = [central_difference(f, xi, dx) for xi in xcoord]\n",
    "    # 三阶差分的结果\n",
    "    dfdx_three_num = [three_order_difference(f, xi, dx) for xi in xcoord]\n",
    "    \n",
    "    plt.plot(xcoord, dfdx_num, 'ko--', label=\"finite difference\")\n",
    "    plt.plot(xcoord, dfdx_cen_num, 'b*--', label=\"central difference\")\n",
    "    plt.plot(xcoord, dfdx_three_num, 'gx--', label=\"three_order_difference\")\n",
    "    plt.plot(xcoord, dfdx_ana, 'r-', label=\"sympy: analytic\")\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(r\"$x$\")\n",
    "    plt.ylabel(r\"$df/dx$\")\n",
    "    \n",
    "# dx小的时候，有限差分比较准确\n",
    "# 移动滑钮看到 dx 大的时候，有限差分误差较大\n",
    "# 同时精度越高，收敛地越快。\n",
    "interact(comparison, dx=(0.001, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，我们也可以得到二阶微分的有限差分公式：\n",
    "只需要(2)+(3)再移项就可以得到：\n",
    "\n",
    "$${df \\over dx}  = {f(x + \\Delta x) - f(x - \\Delta x) \\over (\\Delta x)^2} + O( \\Delta x^3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3efc4bf4c5245be92f5cae52dcf5694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5005, description='dx', max=1.0, min=0.001), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.comparison(dx)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_order_centeral_difference(func, x, dx=0.01):\n",
    "    \"\"\"\n",
    "    o(x^2)\n",
    "    \"\"\"\n",
    "    assert(dx !=0)\n",
    "    return (func(x+dx) + func(x-dx)-2*func(x)) / dx**2\n",
    "\n",
    "df2dx2 = sympy.diff(f(x), x, 2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "\n",
    "# 这里作图看一下有限差分方法的误差\n",
    "def comparison(dx):\n",
    "    xcoord = np.linspace(0, np.pi, 20)\n",
    "    # 解析微分的结果\n",
    "    df2dx2_ana = [df2dx2.subs(x, xi) for xi in xcoord]\n",
    "    # 有限差分的结果\n",
    "    df2dx2_num = [two_order_centeral_difference(f, xi, dx) for xi in xcoord]\n",
    "    \n",
    "    plt.plot(xcoord, df2dx2_num, 'ko--', label=\"finite difference\")\n",
    "    plt.plot(xcoord, df2dx2_ana, 'r-', label=\"sympy: analytic\")\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(r\"$x$\")\n",
    "    plt.ylabel(r\"$df/dx$\")\n",
    "    \n",
    "# dx 小的时候，有限差分比较准确\n",
    "# 移动滑钮看到 dx 大的时候，有限差分误差较大\n",
    "interact(comparison, dx=(0.001, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00000000e-01, 2.16194004e-02, 1.39716990e-04, 4.13701855e-08,\n",
       "       5.40219515e-08, 3.46406653e-09, 1.24941224e-07, 5.80184932e-06,\n",
       "       2.69014627e-04, 1.19115183e-02])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_error(dx=0.1, x0=1.0):\n",
    "    ana = 0.5\n",
    "    num = (np.sqrt(x0 + dx) - np.sqrt(x0))/dx\n",
    "    return np.abs(ana - num)\n",
    "dx = np.logspace(-16, -1, 10)\n",
    "err = check_error(dx)\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'error')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(dx, err, 'ro-')\n",
    "plt.xlabel(\"dx\")\n",
    "plt.ylabel(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在某些函数中，并不是dx越小，有限差分的误差越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**列表函数**\n",
    "\n",
    "对于列表函数，可以先使用插值算法得到插值多项式，然后求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动微分（Automatic differentiation）\n",
    "\n",
    "- 数值微分的缺点：会引入数值误差\n",
    "- 解析微分的缺点：在函数复杂的时候，根据链式规则，展开项很多，计算速度慢\n",
    "- 自动微分的优点：保留了数值微分速度快和解析微分结果精确。\n",
    "\n",
    "自动微分在人工智能时代开始流行，因为 Tensorflow, pytorch 等深度学习库封装了自动微分功能。\n",
    "\n",
    "存在两种模式：\n",
    "- Forward Mode\n",
    "- Backward Mode\n",
    "\n",
    "Forward Mode:\n",
    "\n",
    "引入对偶数（Dual number）的概念。\n",
    "\n",
    "对于对偶数 $\\varepsilon$，有 $\\varepsilon^2=0$ 和 $\\varepsilon \\neq 0$\n",
    "\n",
    "对于某一个函数的麦克劳林展开：\n",
    "\n",
    "$f(x)=a_0+a_1x+a_2x^2+\\cdots$\n",
    "\n",
    "将x替换为 $x+\\dot{x}\\varepsilon$，并代入，有：\n",
    "\n",
    "$f(x)=a_0+a_1(x+\\dot{x}\\varepsilon)+a_2(x^2+2x\\dot{x}\\varepsilon+(\\dot{x}\\varepsilon)^2)+\\cdots$\n",
    "\n",
    "根据先前设置的对偶数计算法则，我们有：\n",
    "\n",
    "$f(x)=a_0+a_1x+a_2x^2+\\cdots+a_1\\dot{x}\\varepsilon+a_2x\\dot{x}\\varepsilon+\\cdots+a_nx^n\\dot{x}\\varepsilon$\n",
    "\n",
    "对于后半部分，实际上可以提取出 $\\dot{x} \\varepsilon (a_1+a_2x+\\cdots)$，括号内的部分是 $f^{\\prime}(x)$\n",
    "\n",
    "那么，我们可以得到 $f(x+\\dot{x} \\varepsilon)=f(x)+\\dot{x} \\varepsilon f^{\\prime}(x)$\n",
    "\n",
    "即 $\\frac{df(x)}{dx}|_{x=\\nu} = \\varepsilon \\ \\mathrm{cofficient} (\\mathrm{dual-version} (f) (\\nu+1\\varepsilon))$ \n",
    "\n",
    "事实上，对偶数可以被看做是一种无穷小，所以 $\\varepsilon \\neq 0$，而 $\\varepsilon^2=0$ 是高阶无穷小，所以等于0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.36787944117144233)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简单举例  f(x) = x e^{-x^2}\n",
    "\n",
    "def f(x):\n",
    "    w1 = x\n",
    "    w2 = w1 * w1\n",
    "    w3 = np.exp(- w2)\n",
    "    w4 = w1 * w3\n",
    "    return w4\n",
    "\n",
    "f(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 自动微分版本\n",
    "\n",
    "def f_ad(x):\n",
    "    w1 = x\n",
    "    dw1 = 1\n",
    "    \n",
    "    w2 = w1 * w1\n",
    "    dw2 = 2 * w1 * dw1\n",
    "    \n",
    "    w3 = np.exp(-w2)\n",
    "    dw3 = - np.exp(-w2) * dw2\n",
    "    \n",
    "    w4 = w1 * w3\n",
    "    dw4 = w1 * dw3 + dw1 * w3\n",
    "    \n",
    "    return w4, dw4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.36787944117144233), np.float64(-0.36787944117144233))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ad(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle -0.367879441171442$"
      ],
      "text/plain": [
       "-0.367879441171442"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx.subs(x, 1.)\n",
    "\n",
    "# dfdx 是前面用 sympy 计算出的解析微分\n",
    "# 两者在小数点后 15 位都保持一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运算符重载的方法实现自动微分编程\n",
    "\n",
    "举例：计算机如何做复数的加减乘除\n",
    "\n",
    "$z_1=x_1+iy_1 \\\\\n",
    "z_2=x_2+iy_2$\n",
    "\n",
    "底层语言只实现了实数的加减乘除，但是使用复数库，可以直接计算\n",
    "\n",
    "$z_1+z_2 \\\\\n",
    " z_1−z_2 \\\\\n",
    " z_1∗z_2 \\\\\n",
    " z_1/z_2$\n",
    "\n",
    "复数库对 +， -， *， / 符号做了重载。而在对偶数运算中，我们同样可以仿效复数的方式对常用的运算符号和初始函数进行进行重载:\n",
    "\n",
    "$sin(x + \\dot{x} \\varepsilon) = sin(x) + cos(x) \\dot{x}\\varepsilon \\\\\n",
    "cos(x + \\dot{x} \\varepsilon) = cos(x) - sin(x)  \\dot{x}\\varepsilon \\\\\n",
    "e^{x + \\dot{x} \\varepsilon} = e^x + e^x \\dot{x} \\varepsilon \\\\\n",
    "\\log(x + \\dot{x} \\varepsilon) = \\log(x) + { \\dot{x}\\over x } \\varepsilon \\quad \\; x \\neq 0 \\\\\n",
    "\\sqrt{x + \\dot{x} \\varepsilon} = \\sqrt{x} + { \\dot{x}\\over 2 \\sqrt{x}}\\varepsilon \\quad \\; x \\neq 0 \\\\\n",
    "(x + \\dot{x} \\varepsilon)^2 = x^2 + 2x \\dot{x} \\varepsilon \\\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DNumber:\n",
    "    '''自动微分中的对偶数类，对 +, -, *, /, 幂次符号做了重载'''\n",
    "    def __init__(self, x, dx):\n",
    "        self.val = x\n",
    "        self.dval = dx\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''使用 print(DNumber(1, 2)) 时输出:1 + 2 d'''\n",
    "        # 魔法方法（Magic methods），有时也被称为特殊方法，\n",
    "        # 是一种具有预定义名称的方法，其特征是在开头和结尾处有双下划线，例如，__init__。\n",
    "        # 它们之所以是 \"魔法\"，是因为这些方法是间接调用的，\n",
    "        # 你不需要直接调用它们，一切都在背后完成。可以通过这些方法重载常见操作符。\n",
    "        # 重载print\n",
    "        return f'{self.val} + {self.dval} d'\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        '''overload a + b'''\n",
    "        # 重载加法运算符，\n",
    "        # 先判断other类型，如果是浮点数或者是整数就直接和已经有的对偶数的第一部分相加。\n",
    "        if isinstance(other, float) or isinstance(other, int):\n",
    "            val = self.val + other\n",
    "            dval = self.dval\n",
    "        # 如果是对偶数则直接对对应部分相加。\n",
    "        if isinstance(other, DNumber):\n",
    "            val = self.val+other.val\n",
    "            dval = self.dval + other.dval\n",
    "            \n",
    "        return DNumber(val, dval)\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        '''overload a += b'''\n",
    "        # 重载i=i+a (i+=a)运算符，\n",
    "        # 只有对偶数才能和对偶数实现迭代加法。\n",
    "        self.val = self.val + other.val\n",
    "        self.dval = self.dval + other.dval\n",
    "        return self\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        ''' overload other * x  or other_const * x\n",
    "        (other + d_other d)*(x + dx d) = other*x + (d_other x + other dx) d'''\n",
    "        # 按照对偶数相乘的运算法则对正常的左乘（左操作数调用）进行重构。\n",
    "        # 同样需要先判断other类型。\n",
    "        if isinstance(other, float) or isinstance(other, int):\n",
    "            val = other * self.val\n",
    "            dval = other * self.dval\n",
    "        \n",
    "        if isinstance(other, DNumber):\n",
    "            val = self.val * other.val\n",
    "            dval = self.val * other.dval + other.val * self.dval\n",
    "            \n",
    "        return DNumber(val, dval)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        ''' overload x * other or x * other_const'''\n",
    "        # 右乘（右操作数调用）\n",
    "        # 如果输入为3*DNumber(1,2)，左边不是DNumber类，必须重载右乘才能进行操作。\n",
    "        # 所以其中3是self，而DNnumber(1,2)为other。\n",
    "        return self.__mul__(other)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        '''overload a / b'''\n",
    "        # 重载除法运算符，\n",
    "        # 先判断other类型，如果是浮点数或者是整数，直接和实数部分相除。\n",
    "        if isinstance(other, float) or isinstance(other, int):\n",
    "            val = self.val / other\n",
    "            dval = self.dval / other\n",
    "            return DNumber(val, dval)\n",
    "        elif isinstance(other, DNumber):\n",
    "            # (x + dx d)/(y + dy d) = x/y + ((dx y - x dy) / y²) d\n",
    "            val = self.val / other.val\n",
    "            dval = (-self.val*other.dval+self.dval*other.val) / (other.val**2)\n",
    "            return DNumber(val, dval)\n",
    "        else:\n",
    "            return NotImplemented\n",
    "        \n",
    "    def __rtruediv__(self, other):\n",
    "        # 重载常数除以对偶数。\n",
    "        if isinstance(other, (float, int)):\n",
    "            val = other / self.val\n",
    "            dval = -other*self.dval / (self.val**2) \n",
    "            return DNumber(val, dval)\n",
    "        else:\n",
    "            return NotImplemented\n",
    "    \n",
    "    def __pow__(self, n):\n",
    "        ''' overload power'''\n",
    "        if isinstance(n, float) or isinstance(n, int):\n",
    "            val = self.val**n\n",
    "            dval = n * self.val**(n-1) * self.dval\n",
    "        # (x1+y1d)**(x2+y2d)\n",
    "        elif isinstance(n, DNumber) :\n",
    "            raise(Exception(\"Pow(DNumber, DNumber) is not implemented yet\"))\n",
    "        return DNumber(val, dval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000000000000004 + 1 d"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = DNumber(0.1, 1)\n",
    "s + DNumber(0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s += DNumber(0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30000000000000004 + 1 d"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010000000000000002 + 0.2 d"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = DNumber(0.1, 1)\n",
    "x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010000000000000002 + 0.030000000000000006 d"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667 + 0.0 d"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = DNumber(0.2, 0.2)\n",
    "y = DNumber(0.3, 0.3)\n",
    "x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0 + -9.999999999999998 d"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = DNumber(0.2, 0.2)\n",
    "2 / x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多个变量的自动微分\n",
    "\n",
    "上面例子中f(x)是x的单变量函数，$\\dot{x}=1$\n",
    "\n",
    "如果是多变量函数f(x, y), 要求对x求微分，则应取\n",
    "\n",
    "$\\dot{x}=1, \\dot{y}=0$\n",
    "\n",
    "举例说明：\n",
    "\n",
    "$f(x, y)= 3 x^2  y + x$\n",
    "\n",
    "${\\partial f \\over \\partial x} = 6 x y + 1$\n",
    "\n",
    "在 $(x, y)=(1, 1)$ 时，$f(x, y)=4.0$，$df/dx = 7.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2d(x, y):\n",
    "    '''\n",
    "    Args: \n",
    "    x: DNumber\n",
    "    y: DNumber\n",
    "    return: f(x, y) = 3*x**2*y + x \n",
    "    and partial f / partial x '''\n",
    "    return 3 * x**2 * y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1.0 + 1.0 d\n",
      "y = 1.0 + 0.0 d\n",
      "3x^2 y + x =  4.0 + 7.0 d\n"
     ]
    }
   ],
   "source": [
    "# 求 (x,y)=(1,1) 时 f(x, y) 与 df/dx\n",
    "x = DNumber(1.0, 1.0)\n",
    "y = DNumber(1.0, 0.0)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"3x^2 y + x = \", f2d(x, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
